%!TEX ROOT=../emnlp2023.tex

\section{Results and analysis}
\label{sec:results}
\label{nothink}

\begin{table}
\centering
\begin{tabular}{l
>{\centering\arraybackslash}p{.7cm} 
>{\centering\arraybackslash}p{.7cm} 
>{\centering\arraybackslash}p{.7cm} 
>{\centering\arraybackslash}p{.7cm} 
>{\centering\arraybackslash}p{.7cm}}
{\small{\textbf{System}}} &
\rotatebox{70}{\textbf{\footnotesize{old AVeriTeC score}}} &
\rotatebox{70}{\textbf{Q only} {\footnotesize{(\evr)}}} &
\rotatebox{70}{\textbf{Q + A} {\footnotesize{(\evr)}}} &
\rotatebox{70}{\textbf{\footnotesize{new AVeriTeC score}}} &
\rotatebox{70}{{\footnotesize{\textbf{time per claim}}}} \\
\hline
{\small{CTU AIC}}       & 0.41 & 0.20 & \textbf{0.48} & \textbf{0.33} & 54\textit{s} \\
{\small{HUMANE}}        & 0.45 & 0.19 & 0.43 & 0.27 & 29\textit{s} \\
{\small{yellow flash}}  & 0.16 & 0.16 & 0.41 & 0.25 & 32\textit{s} \\
{\small{FZIGOT}}        & 0.46 & \textbf{0.36} & 0.40 & 0.24 & 19\textit{s} \\
{\small{EFC}}           & 0.49 & 0.13 & 0.35 & 0.20 & \textbf{~7\textit{s}} \\
{\small{checkmate}}     & 0.38 & 0.18 & 0.34 & 0.20 & 22\textit{s} \\
\hline
{\small{Baseline}}      & \textbf{0.50} & 0.27 & 0.34 & 0.20 & 34\textit{s} \\
\end{tabular}
\caption{\averitec{} shared task system leaderboard as shared by organizers, listing new \evr{}-recall-based~\cite{akhtar2024ev2r} and legacy hu-METEOR AVeriTeC scores. Evaluated using AVeriTeC 2025 test set. Best scores are bold.}
\label{tab:leaderboard}
\end{table}

\begin{table}
\centering
\begin{tabular}{l
>{\centering\arraybackslash}p{1.1cm} 
>{\centering\arraybackslash}p{1.1cm} 
>{\centering\arraybackslash}p{1.1cm}}
\textbf{System} &
\rotatebox{70}{\textbf{Q only} {\footnotesize{(\evr)}}} &
\rotatebox{70}{\textbf{Q + A} {\footnotesize{(\evr)}}} &
\rotatebox{70}{\textbf{\footnotesize{new AVeriTeC score}}} \\
\hline
llama70b (2024)   & 0.37 & 0.54 & 0.39 \\
gpt4o (2024)      & 0.36 & 0.54 & 0.37 \\
qwen3-nothink     & 0.29 & 0.59 & 0.41 \\
qwen3-think       & 0.25 & 0.55 & 0.36 \\
\hline
\end{tabular}
\caption{Ablation study on LLM choice and \texttt{<think>}-tokens impact on \averitec{} score. Pipeline design (Figure~\ref{fig:pipeline}), retrieval results, system and user prompts are fixed to be the same for each model. Evaluated using an on-premise~\evr{} scorer with Ollama-hosted Llama3.3-70B as a judge.}
\label{tab:averitec-ablation}
\end{table}

\subsection{Why does the system perform well?}
\label{sec:why}
One thing we would like to address with this paper is why does our system outperform the \averitec{} baseline and even the other systems submitted to \averitec{} shared task despite the simplicity of its design (Figure~\ref{fig:pipeline}) which boils down to a straightforward case of retrieval-augmented generation (RAG).

The main reason, in our experience, is the large \textbf{context size} we opt for -- while even the \averitec{} baseline processes the claims and sources in a manner more sophisticated than we do, it processes the knowledge store on a \textit{sentence} level, reducing the amount of information passed to the LLM as opposed to working with \textit{documents} as a whole, which is the strategy our system approximates.

Put simply, despite our proposed integration of LLM into the pipeline being rather vanilla, combining sources of total length of as much as 60K characters\footnote{In other words, around 33 standard pages. This number follows from our parameter choices in Section~\ref{sec:choices}: 10 sources are retrieved for each claim, each with $\sim2048$ characters of the embedded text, and additional $\sim4096$ characters of context.} on model input yielded highly competitive results, leveraging its own trained mechanisms of context processing.

Our other advantages may have been using a very recent model, Qwen3~\cite{yang2025qwen3technicalreport}, which naturally has a slightly higher leakage of 2025 claims into its train set than older models, and outperforms the previous LLM generations at long sequence processing. Furthermore, our pipeline design only uses a single LLM call per claim, meaning we could use the generously-sized 14B variant of Qwen3 and still match the time limit with Nvidia A10 and 20GB GRAM.